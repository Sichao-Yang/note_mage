### Dimension reduction

[toc]

ç©ºé—´é™ç»´æŠ€æœ¯ï¼Œé¦–å…ˆæ˜¯çº¿æ€§ç©ºé—´é‡Œçš„PCAï¼Œç„¶åæ˜¯éçº¿æ€§çš„kernel PCAã€‚è¿™é‡Œè€ƒè™‘çš„è§’åº¦æ˜¯æŠŠæ–¹å·®ä¸å¤§çš„ç»´åº¦æ‰”æ‰ï¼Œä½†æ˜¯è¿™æ ·ä¸èƒ½è€ƒè™‘åˆ°ä¸€äº›ç»†å¾®çš„ç©ºé—´ç»“æ„æ˜¯åŸºäºå…¨å±€è€Œéå±€éƒ¨çš„æ–¹æ³•ï¼Œå…¶åº¦é‡å‡½æ•°ä»…é’ˆå¯¹æ¬§å¼ç©ºé—´ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¼•å…¥ISOMAPçš„é»æ›¼æµå½¢çš„æ–¹æ³•ã€‚å½“ç„¶ISOMAPçš„å¼•å…¥å…¶å®å’ŒMDSæ›´ç›´æ¥ç›¸å…³ã€‚å½“è§£å†³äº†å±€éƒ¨ç©ºé—´åº¦é‡åï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘æˆ‘ä»¬æ‰€å­¦ä¹ çš„è¿™ä¸ªç©ºé—´çš„å‡å¸ƒæ€§ï¼Œæ‰€ä»¥æˆ‘ä»¬å†æ¬¡æ”¹è¿›ç®—æ³•å¾—åˆ°UMAPç®—æ³•ã€‚

#### 1 PCA

Principal component analysisï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å¾—åˆ°ï¼Œå°±æ˜¯è¦æ‰¾å‡ºä¸€ä¸ªåæ ‡è½´æ¥æœ€å¤§åŒ–æ–¹å·®ï¼Œä¹Ÿå°±æ˜¯ $max \sum(x)^2$ã€‚è¿™ä¸ªå‘é‡å…¶å®å°±æ˜¯XTXçš„æœ€å¤§å¥‡å¼‚å€¼å¯¹åº”çš„å‘é‡ã€‚

![PCA (Principal Component Analysis) Machine Learning Tutorial](media/DimensionReduction/Principal+Component+Analysis.jpgw=600&dpr=2.6)

[wiki](https://en.wikipedia.org/wiki/Principal_component_analysis)

##### First component

In order to maximize variance, the first weight vector $\mathbf{w}_{(1)}$ thus has to satisfy
$$
\mathbf{w}_{(1)}=\arg \max _{\|\mathbf{w}\|=1}\left\{\sum_{i}\left(t_{1}\right)_{(i)}^{2}\right\}=\arg \max _{\|\mathbf{w}\|=1}\left\{\sum_{i}\left(\mathbf{x}_{(i)} \cdot \mathbf{w}\right)^{2}\right\}
$$
Equivalently, writing this in matrix form gives
$$
\mathbf{w}_{(1)}=\arg \max _{\|\mathbf{w}\|=1}\left\{\|\mathbf{X} \mathbf{w}\|^{2}\right\}=\arg \max _{\|\mathbf{w}\|=1}\left\{\mathbf{w}^{\top} \mathbf{X}^{\top} \mathbf{X} \mathbf{w}\right\}
$$
Since $\mathbf{w}_{(1)}$ has been defined to be a unit vector, it equivalently also satisfies
$$
\mathbf{w}_{(1)}=\arg \max \left\{\frac{\mathbf{w}^{\top} \mathbf{X}^{\top} \mathbf{X} \mathbf{w}}{\mathbf{w}^{\top} \mathbf{w}}\right\}
$$
The quantity to be maximized can be recognized as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as $\mathbf{X}^{\top} \mathbf{X}$ is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when $w$ is the corresponding eigenvector.
With $\mathbf{w}_{(1)}$ found, the first principal component of a data vector $\mathbf{x}_{(i)}$ can then be given as a score $t_{1(i)}=\mathbf{x}_{(i)} \cdot \mathbf{w}_{(1)}$ in the transformed co-ordinates, or as the corresponding vector in the original variables, $\left\{\mathbf{x}_{(i)} \cdot \mathbf{w}_{(1)}\right\} \mathbf{w}_{(1)}$.

##### Further components

The $k$-th component can be found by subtracting the first $k-1$ principal components from $\mathbf{X}:$
$$
\hat{\mathbf{X}}_{k}=\mathbf{X}-\sum_{s=1}^{k-1} \mathbf{X} \mathbf{w}_{(s)} \mathbf{w}_{(s)}^{\top}
$$
and then finding the weight vector which extracts the maximum variance from this new data matrix
$$
\mathbf{w}_{(k)}=\underset{\|\mathbf{w}\|=1}{\arg \max }\left\{\left\|\hat{\mathbf{X}}_{k} \mathbf{w}\right\|^{2}\right\}=\arg \max \left\{\frac{\mathbf{w}^{\top} \hat{\mathbf{X}}_{k}^{\top} \hat{\mathbf{x}}_{k} \mathbf{w}}{\mathbf{w}^{T} \mathbf{w}}\right\}
$$
It turns out that this gives the remaining eigenvectors of $\mathbf{X}^{\top} \mathbf{X}$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of $\mathbf{X}^{\top} \mathbf{X}$.
The $k$-th principal component of a data vector $\mathbf{x}_{(i)}$ can therefore be given as a score $t_{k(t)}=\mathbf{x}_{(i)} \cdot \mathbf{w}_{(k)}$ in the transformed coordinates, or as the corresponding vector in the space of the original variables, $\left\{\mathbf{x}_{(i)} \cdot \mathbf{w}_{(k)}\right\} \mathbf{w}_{(k)}$, where $\mathbf{w}_{(k)}$ is the $k$ th eigenvector of $\mathbf{X}^{\top} \mathbf{X} .$
The full principal components decomposition of $\boldsymbol{X}$ can therefore be given as
$$
\mathbf{T}=\mathbf{X W}
$$
where $\mathbf{W}$ is a $p$-by- $p$ matrix of weights whose columns are the eigenvectors of $\mathbf{X}^{\top} \mathbf{X}$. The transpose of $\mathbf{W}$ is sometimes called the whitening or sphering transformation. Columns of $\mathbf{W}$ multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis.

#### 2 MDS

[wiki](https://en.wikipedia.org/wiki/Multidimensional_scaling)

Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. MDS is used to translate "information about the pairwise 'distances' among a set of $n$ objects or individuals" into a configuration of $n$ points mapped into an abstract Cartesian space.

å®ƒå°±æ˜¯æŠŠå­˜åœ¨é«˜ç»´ç©ºé—´é‡Œçš„ç‚¹éƒ½å‹ç¼©åˆ°ä½ç»´ç©ºé—´çš„åŒæ—¶ä¿ç•™ç‚¹å’Œç‚¹ä¹‹é—´çš„ â€˜è·ç¦»â€™ï¼Œè¿™é‡Œçš„è·ç¦»åœ¨ç»å…¸MDSé‡Œæ˜¯æ¬§å¼çš„ï¼Œåæ¥è¢«æ‹“å±•åˆ°éæ¬§å¼çš„ã€‚

> Steps of a Classical MDS algorithm
>
> Classical MDS uses the fact that the coordinate matrix $X$ can be derived by eigenvalue decomposition from $B=X X^{\prime}$. And the matrix $B$ can be computed from proximity matrix $D$ by using double centering.
>
> 1. Set up the squared proximity matrix $D^{(2)}=\left[d_{i j}^{2}\right]$ï¼Œ$d_{i j}=\sqrt{\left(x_{i}-x_{j}\right)^{2}+\left(y_{i}-y_{j}\right)^{2}}$
> 2. Apply double centering: $B=-\frac{1}{2} C D^{(2)} C$ using the centering matrix $C=I-\frac{1}{n} J_{n}$, where $n$ is the number of objects, $I$ is the $n \times n$ identity matrix, and $J_{n}$ is an $n \times n$ matrix of all ones.
> 3. Determine the $m$ largest eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and corresponding eigenvectors $e_{1}, e_{2}, \ldots, e_{m}$ of $B$ (where $m$ is the number of dimensions desired for the output).
> 4. Now, $X=E_{m} \Lambda_{m}^{1 / 2}$, where $E_{m}$ is the matrix of $m$ eigenvectors and $\Lambda_{m}$ is the diagonal matrix of $m$ eigenvalues of $B$.
>    Classical MDS assumes Euclidean distances. So this is not applicable for direct dissimilarity ratings.

ä¸Šé¢ç›´æ¥ç»™äº†æ–¹æ³•ï¼Œæ²¡æœ‰ç»™æ¨å¯¼ï¼Œ==ä¸å¦‚çœ‹ [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/51441355) è¿™ç¯‡==ã€‚

##### æ¨å¯¼

ç»™å®š $N$ ä¸ªå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹æ˜¯ä¸€ä¸ª $(1 \times m)$ ç»´çš„å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡º $m$ ç»´ç©ºé—´ä¸­çš„è·ç¦»çŸ© é˜µ $D ï¼Œ D$ æ˜¯ä¸€ä¸ª $(N \times N)$ çŸ©é˜µï¼›å…¶ä¸­ç¬¬ $i$ è¡Œ $j$ åˆ—çš„å…ƒç´ è¡¨ç¤ºç¬¬ $i$
ä¸ªå®ä¾‹å’Œç¬¬ $j$ ä¸ªå®ä¾‹ä¹‹é—´çš„è·ç¦»ã€‚å‡è®¾æˆ‘ä»¬æŠŠæ•°æ®é™ç»´è‡³ $Z$ ç»´ç©ºé—´ä¸­ï¼Œå…¶ä¸­ $Z_{i}$
è¡¨ç¤ºå…¶ä¸­ç¬¬ $i$ ä¸ªå®ä¾‹ã€‚

###### åˆ©ç”¨å†…ç§¯å½¢å¼æ¨å¯¼(Classical MDS)

è¦æ±‚ä»»æ„ä¸¤ä¸ªå®ä¾‹åœ¨Zç»´ç©ºé—´ä¸­çš„è·ç¦»ä¸åŸå§‹ç©ºé—´çš„è·ç¦»ç›¸åŒã€‚å› æ­¤æˆ‘ä»¬æœ‰å¦‚ä¸‹è¡¨è¾¾å¼:

$$
d_{i j}{ }^{2}=\left\|z_{i}-z_{j}\right\|^{2}=\left\|z_{i}\right\|^{2}+\left\|z_{j}\right\|^{2}-2 z_{i}^{T} z_{j}
$$

å› ä¸ºå°†åœ¨ $Z$ ç»´ç©ºé—´ä¸­ç©ºé—´ä¸­ï¼Œç‚¹å¯ä»¥è¿›è¡Œå¹³ç§»ä¸æ—‹è½¬ï¼Œå› æ­¤åœ¨Zç»´ç©ºé—´ä¸­ä¼šæœ‰å¤šç§åˆ†å¸ƒæ»¡è¶³è¦ æ±‚ï¼Œä¸å¤±ä¸€èˆ¬æ€§ï¼Œæˆ‘ä»¬å‡è®¾ $Z$ ç»´ç©ºé—´ä¸­çš„å®ä¾‹ç‚¹æ˜¯ä¸­å¿ƒåŒ–çš„ï¼Œå³:
$$
\sum_{i=1}^{N} z_{i}=0		\tag{1}
$$
æˆ‘ä»¬å¯¹å…¬å¼ (1) å·¦å³ä¸¤è¾¹æ±‚å’Œ:
$$
\sum_{i=1}^{N} d_{i j}^{2}=\sum_{i=1}^{N}\left\|z_{i}\right\|^{2}+N\left\|z_{i}\right\|^{2} \tag{2} 
$$

$$
\sum_{j=1}^{N} d_{i j}^{2}=\sum_{j=1}^{N}\left\|z_{j}\right\|^{2}+N\left\|z_{j}\right\|^{2}	\tag{3}
$$
 å¯¹å…¬å¼(3)ä¸¤è¾¹å†æ¬¡è¿›è¡Œæ±‚å’Œ:
$$
\sum_{i=1}^{N} \sum_{j=1}^{N} d_{i j}^{2}=\sum_{i=1}^{N} \sum_{j=1}^{N}\left\|z_{j}\right\|^{2}+N \sum_{i=1}^{N}\left\|z_{i}\right\|^{2}=2 N \sum_{i=1}^{N}\left\|z_{i}\right\|^{2} 	\tag{4}
$$
å®šä¹‰å†…ç§¯çŸ©é˜µ $B=Z^{T} Z$ ï¼Œå…¶ä¸­ã€‚å°†å¼(2)(3)(4)ä»£å…¥å¼(1)ä¸­ï¼Œå¯å¾—:
$$
b_{i j}=-\frac{1}{2}\left(\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=1}^{N} d_{i j}^{2}-\frac{1}{N} \sum_{i=1}^{N} d_{i j}^{2}-\frac{1}{N} \sum_{j=1}^{N} d_{i j}^{2}+d_{i j}^{2}\right) 	\tag{5}
$$
ç”±äºçŸ©é˜µ $B$ æ˜¯ä¸€ä¸ªæ˜¯å¯¹ç§°çŸ©é˜µï¼Œå› æ­¤å¯¹çŸ©é˜µ $B$ è¿›è¡Œç‰¹å¾åˆ†è§£å¯ä»¥å¾—åˆ°:
$$
B=V A V^{T} 
$$
å…¶ä¸­ï¼Œ $\Lambda$ æ˜¯çš„ç‰¹å¾å€¼çŸ©é˜µï¼Œ $E$ æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µã€‚ç”±äºæˆ‘ä»¬å°†æ•°æ®é™ç»´åˆ° $Z$ ç»´ç©ºé—´ä¸­ï¼Œå› æ­¤æˆ‘ ä»¬é€‰æ‹©å‰ $z$ ä¸ªæœ€å¤§çš„ç‰¹å¾å€¼ä»¥åŠç‰¹å¾å‘é‡ã€‚é™ç»´ä¹‹åçš„æ•°æ®ç‚¹è¡¨ç¤ºä¸º
$$
Z=V_{z} \Lambda_{z}^{1 / 2}
$$

###### æ„é€ æŸå¤±å‡½æ•°æ±‚è§£(No-classical MDS\&\&classical MDS)

å½“è·ç¦»æ ‡å‡†ä¸æ˜¯æ¬§å¼è·ç¦»çš„æ—¶å€™ï¼Œæ­¤æ—¶ä¸å­˜åœ¨è§£æè§£ï¼Œéœ€è¦é‡‡ç”¨ä¼˜åŒ–ç®—æ³•çš„å½¢å¼æ±‚è§£ã€‚æˆ‘ä»¬çš„ç›®æ ‡ æ˜¯ä½¿æ•°æ®ç‚¹åœ¨é«˜ç»´å’Œä½ç»´ç©ºé—´ä¸­çš„è·ç¦»å°½å¯èƒ½çš„ç›¸è¿‘ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ„é€ å¦‚ä¸‹ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°çš„å€¼æ¥æ±‚ç‚¹åœ¨ $Z$ ç»´ç©ºé—´çš„åˆ†å¸ƒ:
$$
J=\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=i+1}^{N}\left(\left\|z_{i}-z_{j}\right\|-d_{i j}\right)^{2}
$$

#### 2 t-SNE

> **t-distributed stochastic neighbor embedding** (**t-SNE**) is a [statistical](https://en.wikipedia.org/wiki/Statistical) method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. 

t-SNE ä¹Ÿæ˜¯ä¸€ç§å…¸å‹çš„é™ç»´æ–¹æ³•ï¼Œç›®çš„åœ¨äºdata visualizationã€‚read [Wiki page, Details part](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)ï¼š

é™ç»´çš„ç»Ÿä¸€æ€æƒ³æ˜¯åœ¨ä½ç»´ç©ºé—´é‡Œä¿ç•™é«˜ç»´ç©ºé—´ç‚¹å’Œç‚¹ä¹‹é—´çš„â€œè·ç¦»â€ï¼Œåœ¨t-SNEé‡Œç”¨çš„è·ç¦»æ˜¯ï¼ˆé«˜æ–¯ï¼‰æ¦‚ç‡ã€‚

åŸå§‹**é«˜ç»´ç©ºé—´** - é¦–å…ˆæˆ‘ä»¬å®šä¹‰ä»¥iç‚¹ä¸ºä¸­å¿ƒï¼Œå¯¹åº” j æ•°æ®ç‚¹çš„æ¡ä»¶æ¦‚ç‡è®¡ç®—å…¬å¼ä¸ºï¼š
$$
\begin{equation}
p_{j \mid i}=\frac{\exp \left(-\left\|\mathbf{x}_{i}-\mathbf{x}_{j}\right\|^{2} / 2 \sigma_{i}^{2}\right)}{\sum_{k \neq i} \exp \left(-\left\|\mathbf{x}_{i}-\mathbf{x}_{k}\right\|^{2} / 2 \sigma_{i}^{2}\right)}
\end{equation}
$$
ç„¶åå®šä¹‰ (i,j) ç‚¹çš„è”åˆæ¦‚ç‡ä¸ºï¼š
$$
p_{i j}=\frac{p_{j \mid i}+p_{i \mid j}}{2 N}
$$
This is motivated because $p_i$ and $p_j$ from the $\mathrm{N}$ samples are estimated as $1 / \mathrm{N}$, so the conditional probability can be written as $p_{i \mid j}=N p_{i j}$ and $p_{j \mid i}=N p_{j i}$. Since $p_{i j}=p_{j i}$, you can obtain previous formula.

Also note that $p_{i i}=0$ and $\sum_{i, j} p_{i j}=1$

**é™ç»´ç©ºé—´**é‡Œçš„æ¦‚ç‡åˆ†å¸ƒæˆ‘ä»¬é€‰äº†ä¸€ä¸ªdof=1çš„èƒ–å°¾student t dist.æ¨¡å‹ï¼ˆç­‰ä»·äºæŸ¯è¥¿åˆ†å¸ƒæ¨¡å‹ï¼‰å®šä¹‰ä¸ºqåˆ†å¸ƒï¼š
$$
\begin{equation}
q_{i j}=\frac{\left(1+\left\|\mathbf{y}_{i}-\mathbf{y}_{j}\right\|^{2}\right)^{-1}}{\sum_{k} \sum_{l \neq k}\left(1+\left\|\mathbf{y}_{k}-\mathbf{y}_{l}\right\|^{2}\right)^{-1}}
\end{equation}
$$
**ä¼˜åŒ–ç›®æ ‡**ï¼šæ‹‰è¿‘é«˜ä½ç»´ç©ºé—´é‡Œçš„åˆ†å¸ƒè·ç¦»ï¼Œæˆ–è€…è¯´ä¿ç•™é«˜ç»´ç©ºé—´é‡Œçš„åº¦é‡ã€‚å»ºæ¨¡ä¸º æœ€å°åŒ–på’Œqçš„KL divergenceï¼Œç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
$$
\begin{equation}
\mathrm{KL}(P \| Q)=\sum_{i \neq j} p_{i j} \log \frac{p_{i j}}{q_{i j}}
\end{equation}
$$
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/T-SNE_Embedding_of_MNIST.png/1024px-T-SNE_Embedding_of_MNIST.png" alt="undefined" style="zoom:50%;" />

<center>MINST æ•°æ®çš„t-SNEæ•ˆæœ</center>

#### 3 ISOMAP

[wiki](https://en.wikipedia.org/wiki/Isomap )   [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/60599491)

The algorithm provides a simple method for estimating the intrinsic geometry of a data [manifold](https://en.wikipedia.org/wiki/Manifold) based on a rough estimate of each data pointâ€™s neighbors on the manifold. Isomap æ˜¯MDSçš„ä¸€ä¸ªè‡ªç„¶å»¶ä¼¸ï¼Œå®ƒçš„æ–¹æ³•é‡Œï¼Œæ ·æœ¬ç‚¹é—´çš„ç›¸ä¼¼åº¦/åº¦é‡æ–¹å¼ä¸å†æ˜¯æ¬§å¼çš„ï¼Œè€Œæ˜¯åœ¨æµå½¢ä¸Šçš„è·ç¦»ã€‚æµå½¢çš„æ„é€ æ–¹å¼æ˜¯nearest-neighbourã€‚

æ‰€ä»¥å®ƒå’Œä¸‹é¢çš„UMAPéƒ½æ˜¯ä¸¤æ­¥èµ°ï¼šç¬¬ä¸€æ­¥é€šè¿‡æœ€è¿‘é‚»ç®—æ³•æ¥æ„é€ è¿‘ä¼¼æµå½¢å›¾ï¼Œç„¶åå†æŠŠç‚¹å‹ç¼©æ˜ å°„åˆ°æµå½¢ä¸Šã€‚åªä¸è¿‡UMAPé‡Œåšäº†uniform distributionçš„å‡è®¾ã€‚

##### high-level Algorithm

A very high-level description of **Isomap** algorithm is given below.

- Determine the neighbors of each point.
  - All points in some fixed radius.
  - *K* nearest neighbors.
- Construct a neighborhood graph.
  - Each point is connected to other if it is a *K* nearest neighbor.
  - Edge length equal to Euclidean distance.
- Compute shortest path between two nodes.
  - [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra's_algorithm)
  - [Floydâ€“Warshall algorithm](https://en.wikipedia.org/wiki/Floydâ€“Warshall_algorithm)
- Compute lower-dimensional embedding.
  - [Multidimensional scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling)

Isomapçš„ç»å…¸ä¸‰å¼ å›¾ï¼š

è™šçº¿æ˜¯euclidean distanceï¼Œä½†æ˜¯å®ƒåœ¨æ•°æ®çš„æµå½¢ç»“æ„ä¸Šæ˜¯ä¸æˆç«‹çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦æ‰¾çš„åº¦é‡æ–¹å¼æ˜¯geodesicçš„è·ç¦»ä¹Ÿå°±æ˜¯å›¾äºŒï¼Œå…·ä½“è®¡ç®—æœ€çŸ­è·ç¦»çš„æ–¹å¼å°±æ˜¯è¿‘é‚»å›¾ä¸Šçš„æœ€çŸ­è·¯å¾„äº†ã€‚

![img](media/DimensionReduction/v2-86ccf7999ed271976a302eda882bfb3c_720w.jpg)

![img](media/DimensionReduction/v2-8c63b6f9041812d91023b47dd50b3e34_720w.jpg)

![img](media/DimensionReduction/v2-5989530643e01916e9e7b4e7efebefb6_720w.jpg)

#### 4 UMAP

æœ€åç»ˆäºæ¥åˆ°äº†æµè¡Œçš„å‰æ²¿ - UMAPï¼Œè¯¥ç®—æ³•åœ¨

[åŸç‰ˆ](https://towardsdatascience.com/umap-dimensionality-reduction-an-incredibly-robust-machine-learning-algorithm-b5acb01de568)  [ä¸­æ–‡ç‰ˆ](https://zhuanlan.zhihu.com/p/432805218)

- **Projection** â€” the process or technique of reproducing a spatial object upon a plane, a curved surface, or a line by projecting its points. You can also think of it as a mapping of an object from high-dimensional to low-dimensional space.
- **Approximation** â€” the algorithm assumes that we only have a finite set of data samples (points), not the entire set that makes up the manifold. Hence, we need to approximate the manifold based on the data available.
- **Manifold** â€” a manifold is a topological space that locally resembles Euclidean space near each point. One-dimensional manifolds include lines and circles, but not figure eights. Two-dimensional manifolds (a.k.a. surfaces) include planes, spheres, torus, and more.
- **Uniform** â€” the uniformity assumption tells us that our data samples are uniformly (evenly) distributed across the manifold. In the real world, however, this is rarely the case. Hence, this assumption leads to the notion that the distance varies across the manifold. i.e., the space itself is warping: stretching or shrinking according to where the data appear sparser or denser.

*A dimensionality reduction technique that assumes the available data samples are evenly (***uniformly***) distributed across a topological space (***manifold***), which can be* **approximated** *from these finite data samples and mapped (***projected***) to a lower-dimensional space.*

##### High-level steps

We can split UMAP into two major steps:

1. learning the manifold structure in the high-dimensional space;
2. finding a low-dimensional representation of said manifold.

We will, however, break this down into even smaller components to make our understanding of the algorithm deeper. The below map shows the order that we will go through in analyzing each piece.

![img](media/DimensionReduction/1hjo4XLAm_ww93DPYe3aSVQ.png)

###### Step 1 â€” Learning the manifold structure

It will come as no surprise, but before we can map our data to lower dimensions, we first need to figure out what it looks like in the higher-dimensional space.

**1.1. Finding nearest neighbors**
UMAP starts by finding the nearest neighbors using the [Nearest-Neighbor-Descent algorithm of Dong et al](http://www.cs.princeton.edu/cass/papers/www11.pdf). You will see in the Python section later on that we can specify how many nearest neighbors we want to use by adjusting UMAPâ€™s ***n_neighbors\*** hyperparameter.

It is important to experiment with the number of ***n_neighbors\*** because it **controls how UMAP balances local versus global structure in the data**. It does it by constraining the size of the local neighborhood when attempting to learn the manifold structure.

Essentially, a small value for ***n_neighbors\*** means that we want a very local interpretation that accurately captures the fine detail of the structure. In contrast, a large ***n_neighbors\*** value means that our estimates will be based on larger regions, thus more broadly accurate across the manifold as a whole.

**1.2. Constructing a graph**
Next, UMAP needs to construct a graph by connecting the previously identified nearest neighbors. To understand this process, we need to look at a few sub-components that explain how the neighborhood graph comes to be.

**1.2.1. Varying distance**
As outlined in the analysis of the UMAPâ€™s name, we assume a uniform distribution of points across the manifold, suggesting that space between them is stretching or shrinking according to where the data appears to be sparser or denser.

It essentially means that the distance metric is not universal across the whole space, and instead, it varies between different regions. We can visualize it by drawing circles/spheres around each data point, which appear to be different in size because of the varying distance metric (see illustration below).

![img](media/DimensionReduction/1NDTdukI_hbf7J-z2hShz_g-16401777811168.png)

Local connectivity and fuzzy open sets. Image source: [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).

**1.2.2. Local connectivity**
Next, we want to ensure that the manifold structure we are trying to learn does not result in many unconnected points. Luckily, we can use another hyperparameter called ***local_connectivity\*** (default value = 1) to solve this potential problem.

When we set ***local_connectivity=1,\*** we tell the algorithm that every point in the higher-dimensional space is connected to at least one other point. You can see in the above illustration how each solid circle touches at least one data point.

**1.2.3. Fuzzy area**
You must have noticed that the illustration above also contains fuzzy circles extending beyond the closest neighbor. This tells us that the certainty of connection with other points decreases as we get farther away from the point of interest.

The easiest way to think about it is by viewing the two hyperparameters (***local_connectivity\*** and ***n_neighbors\***) as lower and upper bounds:

- **local_connectivity (default=1)** â€” there is 100% certainty that each point is connected to at least one other point (lower limit for a number of connections).
- **n_neighbors (default=15)** â€” there is a 0% chance that a point is directly connected to a 16th+ neighbor since it falls outside the local area used by UMAP when constructing a graph.
- **neighbors 2 to 15** â€” there is some level of certainty (>0% but <100%) that a point is connected to its 2nd to 15th neighbor.

![image-20211222205706614](media/DimensionReduction/image-20211222205706614.png)

**1.2.4. Merging of edges**
Finally, we need to understand that the connection certainty discussed above is expressed through edge weights (***w\***).

Since we have employed a varying distance approach, we will unavoidably have cases where edge weights do not align when viewed from the perspective of each point. E.g., the edge weight for points Aâ†’ B will be different from the edge weight of Bâ†’ A.

![img](media/DimensionReduction/1mO-hgV2ZxR7-NTqlPYZ-uQ-164017778111612.png)

UMAP overcomes the problem of disagreeing edge weights we just described by taking a union of the two edges. Here is how [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) explains it:

> If we want to merge together two disagreeing edges with weight *a* and *b* then we should have a single edge with combined weight ğ‘+ğ‘âˆ’ğ‘â‹…ğ‘. The way to think of this is that the weights are effectively the probabilities that an edge (1-simplex) exists. The combined weight is then the probability that at least one of the edges exists.

In the end, we get a connected neighborhood graph that looks like this:

![img](media/DimensionReduction/1tZ-Z481ITAoWLDVoNMSOzA-164017778111614.png)

*Graph with combined edge weights.*Image source: [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).

###### Step 2 â€” Finding a low-dimensional representation

After learning the approximate manifold from the higher-dimensional space, the next step for UMAP is to project it (map it) to a lower-dimensional space.

**2.1. Minimum distance
**Unlike the first step, we do not want varying distances in the lower-dimensional space representation. Instead, we want the distance on the manifold to be standard Euclidean distance with respect to the global coordinate system.

The switch from varying to standard distances also impacts the proximity to nearest neighbors. Hence, we must pass another hyperparameter called ***min_dist\*** *(default=0.1)* to define the minimum distance between embedded points.

Essentially, we can control the minimum spread of points, avoiding scenarios with many points sitting on top of each other in the lower-dimensional embedding.

**2.2. Minimizing the cost function (Cross-Entropy)
**With the minimum distance specified, the algorithm can start looking for a good low-dimensional manifold representation. UMAP does it by minimizing the following cost function, also known as Cross-Entropy (CE):

![img](media/DimensionReduction/1Uv7z-el3YSutoonlVal6Lw-164017778111616.png)

As you can see, the ultimate goal is to **find the optimal weights of edges in the low-dimensional representation**. These optimal weights emerge as the above Cross-Entropy cost function is minimized following an iterative stochastic gradient descent process.

And that is it! The UMAPâ€™s job is now complete, and we are given an array containing the coordinates of each data point in a specified lower-dimensional space.

#### 5 Laplacian Eigenmaps

##### å¼•å…¥ Laplacian matrix

é¦–å…ˆæˆ‘ä»¬è¦å¯¹Laplacian matrixæœ‰ä¸€ä¸ªæ¦‚å¿µã€‚

æˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨ $G(V, E)$ è¡¨ç¤ºä¸€ä¸ªå›¾ï¼Œå…¶ä¸­ $V$ è¡¨ç¤ºå›¾ä¸­çš„é¡¶ç‚¹ $\left\{v_{1}, v_{2}, \ldots \ldots, v_{n}\right\}  \in \mathbb{R}^{l}ï¼Œ E$ è¡¨ç¤ºé¡¶ç‚¹å’Œé¡¶ç‚¹ä¹‹é—´çš„è¾¹ã€‚å®šä¹‰ $w_{i j}$ ä¸ºé¡¶ç‚¹ $v_{i}$ å’Œ $v_{j}$ ä¹‹é—´çš„æƒé‡ã€‚å¯¹äºä¸€ä¸ªæœ‰ $n$ ä¸ªèŠ‚ç‚¹ çš„å›¾ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸º $(n, n)$ çš„æƒé‡çŸ©é˜µ $W$ ã€‚æ ¹æ®å›¾ä¸­åº¦çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°å›¾çš„ åº¦çŸ©é˜µ $D ï¼Œ D$ æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå…¶ä¸­ $d_{i i}$ è¡¨ç¤ºé¡¶ç‚¹ $v_{i}$ çš„åº¦ã€‚å…¶ä¸­ï¼š
$$
d_{i i}=\sum_{j} w_{i j}
$$
ç„¶åæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µå°±æ˜¯ L = D - Wã€‚å®ƒçš„ä¸€ä¸ªæœ‰ç”¨çš„å…¬å¼ä¸ºï¼š
$$
f^{t} L f=\frac{1}{2}\left(\sum_{i} f_{i}^{2} D i i+\sum_{i} f_{j}^{2} D j j-2 \sum_{i j} f_{i} f_{j} w_{i j}\right)=\frac{1}{2}\left(\sum_{i j}\left(f_{i}^{2}+f_{j}^{2}-2 f_{i} f_{j}\right) w_{i j}\right)=\frac{1}{2} \sum_{i j}\left(f_{i}-f_{j}\right) w_{i j}	\tag 1
$$

##### LEç®—æ³•

LEç®—æ³•æ˜¯ä¸€ç§ä¿ç•™æ•°æ®å±€éƒ¨ç‰¹å¾çš„æµå½¢é™ç»´ç®—æ³•ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯åœ¨ä½ç»´ç©ºé—´å†…å°½å¯èƒ½ä¿ç•™æ•°æ®å±€éƒ¨ç¾˜æœ¬ç‚¹ä¹‹é—´çš„ç»“æ„ä¸å˜ã€‚å‡è®¾åŸå§‹æ•°æ®é›†ä¸º $X=x_{1}, x_{2}, \ldots \ldots x_{n} \in R^{m}$ ï¼Œä¸ºä¸€ä¸ª $(n, m)$ çš„æ•°æ®é›†ã€‚

###### 1 é«˜ç»´ç©ºé—´æ ·æœ¬ç»“æ„æè¿°

åœ¨é«˜ç»´ç©ºé—´ä½¿ç”¨æ„å»º kè¿‘é‚»å›¾ çš„æ–¹æ³•æ¥æè¿°å±€éƒ¨æ ·æœ¬ç‚¹ä¹‹é—´çš„ç»“æ„ã€‚å¯¹äºæ ·æœ¬ç‚¹ $x_{i}$, å¦‚æœæ ·æœ¬ ç‚¹ $x_{j}$ äºå…¶äº’ä¸ºè¿‘é‚»ï¼Œåˆ™æœ‰:
$$
w_{i j}=\exp ^{-\left\|x_{i}-x_{j}\right\|^{2} / t}
$$
å…¶ä¸­ $t$ æ˜¯ä¸€ä¸ªè‡ªå·±æŒ‡å®šçš„å¸¸æ•°ã€‚å¦‚æœ $x_{l}$ ä¸æ˜¯å…¶è¿‘é‚»ï¼Œåˆ™
$$
w_{i l}=0
$$
æœ€åå¾—åˆ°çš„ $W$ çŸ©é˜µå°±æ˜¯é«˜ç»´ç©ºé—´æ ·æœ¬ç‚¹çš„ç»“æ„çŸ©é˜µã€‚ç”±äºè¿™ä¸ªç»“æ„çŸ©é˜µåªå­˜æ”¾æ¯ä¸ªç‚¹å’Œå…¶ kè¿‘é‚»ç‚¹çš„ç»“æ„å…³ç³»ï¼Œ(ä¸æ˜¯Kè¿‘é‚»çš„ç‚¹ï¼Œæƒé‡å€¼é€šé€šä¸º 0 )ï¼Œæ‰€ä»¥LEç®—æ³•åªèƒ½ä¿ç•™æ•°æ®çš„å±€éƒ¨ç»“æ„ã€‚

###### 2 ä½ç»´ç©ºé—´æ ·æœ¬ä¿ç•™ç»“æ„

æˆ‘ä»¬è®¾é™ç»´ä¹‹åçš„çŸ©é˜µä¸º $Y=y_{1}, y_{2}, \ldots \ldots y_{n} \in \mathbb{R}^{m}$ ï¼Œæ˜¯ä¸€ä¸ª $(n, m)$ çš„æ•°æ®é›†ï¼ˆå…¶ä¸­mæ˜¯é™ç»´åçš„ç»´åº¦ï¼Œnæ˜¯æ•°æ®ç‚¹çš„ä¸ªæ•°ï¼‰ï¼Œä¸ºäº†åœ¨ä½ç»´ç©ºé—´ä¸­ä¹Ÿä¿ç•™é«˜ç»´çš„ç»“æ„æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å¦‚ä¸‹çš„ç›®æ ‡å‡½æ•°ï¼š
$$
\sum_{i j}\left(y_{i}-y_{j}\right)^{2} w_{i j}
$$
ä¸Šè¿°å‡½æ•°çš„è¡¨ç¤ºå¦‚æœåœ¨é«˜ç»´ç©ºé—´ä¸­ä¸¤ç‚¹è·ç¦»è¶Šè¿‘ï¼Œåˆ™åœ¨ä½ç»´ç©ºé—´ä¸­ä¸¤ç‚¹çš„è·ç¦»ä¹Ÿè¶Šè¿‘ï¼Œè¿™æ ·å¯ä»¥åœ¨ä½ç»´ç©ºé—´å°½å¯èƒ½çš„ä¿ç•™é«˜ç»´ç©ºé—´æ ·æœ¬ç‚¹çš„ç»“æ„ã€‚æ ¹æ®æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„æ€§è´¨å¯å¾— :
$$
\sum_{i j}\left(y_{i}-y_{j}\right)^{2} w_{i j}=\operatorname{tr}\left(2 Y^{T} L Y\right)
$$
ä¸ºäº†æ¶ˆé™¤ä½ç»´ç©ºé—´ä¸­çš„ç¼©æ”¾å› å­ï¼Œä¹Ÿä¸ºäº†ä¿è¯ $d_{i i}$ å€¼è¾ƒå¤§çš„æ ·æœ¬ç‚¹åœ¨ä½ç»´ç©ºé—´ä¸­æ›´ä¸ºé‡è¦ï¼Œ æ·»åŠ å¦‚ä¸‹é™åˆ¶æ¡ä»¶ :
$$
Y^{T} D Y=I
$$

###### 3 ä¼˜åŒ–ç›®æ ‡

$$
\arg \min _{Y} tr(Y^{T} L Y) \\
Y^{T} D Y=I
$$

æ­¤æ—¶é€šè¿‡lagrange multiplierå¯ä»¥å°†ä¼˜åŒ–ç›®æ ‡å‡½æ•°é—®é¢˜è½¬æ¢æˆä¸ºäº†å¹¿ä¹‰ç‰¹å¾å€¼åˆ†è§£é—®é¢˜ ï¼ˆä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œä¸‹é¢è¿™é‡Œæˆ‘ä»¬å‡å®šYæ˜¯1ç»´çš„ï¼Œä¹Ÿå°±æ˜¯ m=1ï¼ŒY=nxmï¼‰:
$$
A=y^TLy - \lambda(y^TDy-1)  \Rightarrow\\
\frac{\partial A}{\partial y}=Ly=\lambda Dy=0 \Rightarrow\\
L y=\lambda D y \Rightarrow D^{-1} L y=\lambda y
$$
å› æ­¤æˆ‘ä»¬å¯¹çŸ©é˜µ $D^{-1} L$ è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå–å…¶ç¬¬ 2 å°åˆ°ç¬¬ $\mathrm{m}$ å°ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡å³ä¸ºæ‰€æ±‚çš„é™ç»´åçš„ $\boldsymbol{Y}$ ã€‚(å› ä¸ºè¿™é‡Œå‡è®¾m=1ï¼Œæ‰€ä»¥æœ€åè§£å°±æ˜¯y=ç¬¬äºŒå°çš„eigenvalueå¯¹åº”çš„eigen vector)ã€‚

LEçš„ç®—æ³•æµç¨‹å¦‚ä¸‹:

1. è®¾ç½®kå€¼ï¼Œæ„å»ºk-è¿‘é‚»å›¾ ; è®¡ç®—è·å¾—æƒé‡çŸ©é˜µ $\boldsymbol{W}$ å’Œåº¦çŸ©é˜µ $\boldsymbol{D}$ ã€‚
2. å¯¹çŸ©é˜µ $\boldsymbol{D}^{-1} \boldsymbol{L}$ è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œä»ç¬¬äºŒå°å¼€å§‹å–mä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ç»„æˆçš„é™ç»´ä¹‹åçš„ $\boldsymbol{Y}$ ã€‚

##### è®¨è®º

ä¸Šè¿°æè¿°é‡Œéœ€è¦è®¨è®ºçš„æœ‰ä¸¤ç‚¹ï¼š

1. ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦æœ€å°åŒ– $\sum_{i j}\left(y_{i}-y_{j}\right)^{2} w_{i j}$ ?
2. $Y^{T} D Y=I$ æ€ä¹ˆå°±èƒ½æ¶ˆé™¤ä½ç»´ç©ºé—´çš„ç¼©æ”¾å› å­ï¼Ÿ
3. ä¸ºä»€ä¹ˆè¦ä»ç¬¬2å°çš„eigenvalueå¯¹åº”çš„eigen vectoré€‰èµ·ï¼Ÿ

å¯¹äºè¿™ä¸‰ä¸ªé—®é¢˜ï¼Œåœ¨ä½œè€…çš„[åŸè®ºæ–‡](https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf)é‡Œéƒ½æœ‰å¾ˆå¥½çš„æè¿°ã€‚æ€»ç»“å¦‚ä¸‹ï¼š

1. ojb func.çš„è®¾è®¡ä¿è¯å¦‚æœkè¿‘é‚»çš„Yé€‰çš„ä¸å¤Ÿè¿‘ï¼Œlosså°±ä¼šå¾ˆé«˜ï¼šThe objective function with our choice of weights $W_{i j}$ incurs a heavy penalty if neighboring points $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$ are mapped far apart. Therefore, minimizing it is an attempt to ensure that if $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$ are "close," then $y_{i}$ and $y_{j}$ are close as well. 

2. é¦–å…ˆï¼Œè®ºæ–‡çš„ç®—æ³•æ˜¯ä»1ç»´çš„yå¼•å…¥çš„y=(y0,y1,y2...)ï¼Œè¿™æ®µè¯çš„æ„æ€ç›´è§‰ä¸Šæ¥çœ‹å°±æ˜¯é¦–å…ˆé€šè¿‡è¿™ä¸ªçº¦æŸæˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰yçš„scalingçš„é—®é¢˜è§£å†³ï¼ˆå¦åˆ™è¦minimizeé—®é¢˜æˆ‘ä»¬åªéœ€è¦è®©æ‰€æœ‰yéƒ½æ˜ å°„åˆ°æ— é™æ¥è¿‘0ä¸å°±å¥½äº†å˜›ï¼‰ï¼Œå…¶æ¬¡yè¶Šé‡è¦å®ƒåŠ æƒçš„æƒé‡è¶Šå¤§ï¼Œé‚£æˆ‘ä»¬å°±è¶Šå¯èƒ½æŠŠå®ƒæ”¾åœ¨ä½çº¬ç©ºé—´é‡Œæ¯”è¾ƒé è¿‘ä¸­å¿ƒçš„ä½ç½®ã€‚ The constraint $\mathbf{y}^{T} D \mathbf{y}=1$ removes an arbitrary scaling factor in the embedding. Matrix $D$ provides a natural measure on the vertices of the graph. The bigger the value $D_{i i}$ (corresponding to the $i$ th vertex) is, the more "important" is that vertex.

   ç„¶åå¯¹äºmç»´åº¦çš„é—®é¢˜ä¸Šè¿°çº¦æŸå°±å˜æˆäº†$Y^{T} D Y=I$ï¼Œæ‹†å¼€æ¥çœ‹ï¼Œå‘ç°æˆ‘ä»¬ä¸ä»…è¦æ±‚yè‡ªå·±çš„åŠ æƒvarianceè¦å›ºå®šä¸º1ï¼Œè¿˜è¦æ±‚ä¸åŒyçš„covarianceä¹Ÿè¦ç­‰äº0. è¿™ä¹Ÿæ˜¯ä¸ºäº†ä¿è¯æ‰€æœ‰ç‚¹ä¸ä¼šcollapseã€‚å¦‚æœæ˜¯1ç»´çš„ï¼Œæ²¡æœ‰è¿™ä¸ªæ¡ä»¶çš„è¯æ‰€æœ‰ç‚¹ä¼šcollapseåˆ° (1,1,1,1,1,1...)ï¼Œè§ç¬¬ä¸‰ç‚¹ã€‚

3. å› ä¸ºD^-1^Læ˜¯åŠæ­£å®šçŸ©é˜µï¼ˆå¯è¯æ˜ä¸è¿‡ä¹Ÿå¯ä»¥ä¸¾ä¸ªç‰¹ä¾‹ï¼šy=1ï¼‰æ‰€ä»¥å®ƒçš„æœ€å°eigenvalueä¸€å®šæ˜¯0ï¼Œä½†æ˜¯è¿™æ ·æ‰€æœ‰çš„nä¸ªç‚¹å°±éƒ½collapseåˆ°1ä¸ªç‚¹ä¸Šäº†ï¼Œæ²¡æœ‰æ„ä¹‰ã€‚ Let $\mathbf{1}$ be the constant function taking 1 at each vertex. It is easy to see that $\mathbf{1}$ is an eigenvector with eigenvalue 0 . If the graph is connected, 1 is the only eigenvector for $\lambda=0$. To eliminate this trivial solution, which collapses all vertices of $G$ onto the real number 1 , we put an additional constraint of orthogonality and look for
   $$
   \mathbf{y}^{T} D \mathbf{1}=0
   $$

	Thus, the solution is now given by the eigenvector with the smallest nonzero eigenvalue. The condition $\mathbf{y}^{T} D \mathbf{1}=0$ can be interpreted as removing a translation invariance in $\mathbf{y}$. æˆ–è€…è¯´æˆ‘ä»¬ä¸å…è®¸yå’Œ $\mathbf{1}$ ä¹‹é—´æœ‰covarianceã€‚

#### 6 LLE

local linear embeddingï¼Œä¹Ÿå±äºæµå½¢å­¦ä¹ ã€‚å®ƒçš„embeddingæ˜¯å±€éƒ¨çº¿æ€§çš„ï¼Œå¯¹æ¯”ä¸Šé¢çš„LEï¼Œå®ƒçš„distance measureä¸æ˜¯expï¼Œè€Œæ˜¯çº¿æ€§åŠ æƒã€‚æ‰€ä»¥ç›¸å¯¹ç®€å•äº›ï¼Œä¾‹å¦‚ï¼š$x_{1}=w_{12} x_{2}+w_{13} x_{3}+w_{14} x_{4}$

è€æ ·å­ï¼Œæˆ‘ä»¬å…ˆæ„å»ºå‡ºknn graphï¼Œç„¶åé€šè¿‡minimizeä¸‹å¼æ¥è·å¾—Wçš„æƒé‡ï¼ˆæ€»å…±mä¸ªæ ·æœ¬ç‚¹ï¼Œkä¸ªæƒé‡ï¼Œå¯¹äºä»»æ„æ ·æœ¬ç‚¹iï¼Œkä¸ªæƒé‡çš„suméƒ½è¦ä¸º1ï¼‰ï¼š
$$
\mathcal{E}(W)=\sum_{i}\left|\vec{X}_{i}-\sum_{j} W_{i j} \vec{X}_{j}\right|^{2}\\
s.t. \sum_{j} w_{i j}=W_{i}^{T} 1_{k}=1 \tag1
$$
æ¥ç€æŠŠé«˜ç»´ç©ºé—´çš„Xå‹ç¼©åˆ°ä½ç»´ç©ºé—´Yé‡Œå»ï¼Œé€šè¿‡minimizeä¸‹å¼æ¥è·å¾—Yï¼š
$$
\Phi(Y)=\sum_{i}\left|\vec{Y}_{i}-\sum_{j} W_{i j} \vec{Y}_{j}\right|^{2}		\tag2
$$

##### æ±‚è§£

###### å…¬å¼1

$$
\begin{aligned}
J(W) &=\sum_{i=1}^{m}\left\|x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} \\
&=\sum_{i=1}^{m}\left\|\sum_{j \in Q(i)} w_{i j} x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} \\
&=\sum_{i=1}^{m}\left\|\sum_{j \in Q(i)} w_{i j}\left(x_{i}-x_{j}\right)\right\|_{2}^{2} \\
&=\sum_{i=1}^{m} W_{i}^{T}\left(X_{i}-X_{k}\right)\left(X_{i}-X_{k}\right)^{T} W_{i}
\end{aligned}
$$

ä»¤çŸ©é˜µ $Z_{i}=\left(X_{i}-X_{k}\right)\left(X_{i}-X_{k}\right)^{T}$ï¼Œå…¶ä¸­$X_{i}$ ä¸ºkxdçš„çŸ©é˜µï¼ˆæŠŠxiåœ¨0ç»´ä¸Šå¤åˆ¶kéï¼‰ï¼Œ $X_{k}$ ä¸ºkxdçš„çŸ©é˜µï¼ˆxiçš„kä¸ªè¿‘é‚»ï¼‰ï¼Œå¼•å…¥lagrange multiplierï¼š
$$
L(W)=\sum_{i=1}^{m} W_{i}^{T} Z_{i} W_{i}+\lambda\left(W_{i}^{T} 1_{k}-1\right) \\
\frac{\partial L}{\partial W_i} = 2 Z_{i} W_{i}+\lambda 1_{k}=0 \\
W_{i}=\lambda^{\prime} Z_{i}^{-1} 1_{k}
$$
å…¶ä¸­ $\lambda^{\prime}=-\frac{1}{2} \lambda$ ä¸ºä¸€ä¸ªå¸¸æ•°ã€‚åˆ©ç”¨ $W_{i}^{T} 1_{k}=1$, å¯¹ $W_{i}$ å½’ä¸€åŒ–ï¼Œé‚£ä¹ˆæœ€ç»ˆæˆ‘ä»¬çš„æƒé‡ç³»æ•° $W_{i}$ ä¸º:
$$
W_{i}=\frac{Z_{i}^{-1} 1_{k}}{1_{k}^{T} Z_{i}^{-1} 1_{k}}
$$


###### å…¬å¼2

ä¸ºäº†ä½¿å¾—æœ‰å”¯ä¸€å¯è¡Œè§£ï¼Œæˆ‘ä»¬æ·»åŠ ä¸¤ä¸ªçº¦æŸæ¡ä»¶ï¼ˆä¸€ä¸ªæ˜¯ä¸­å¿ƒåŒ–ï¼Œä¸€ä¸ªæ˜¯å½’ä¸€åŒ–ï¼‰
$$
J(y)=\sum_{i=1}^{m}\left\|y_{i}-\sum_{j=1}^{m} w_{i j} y_{j}\right\|_{2}^{2}\\
s.t. \sum_{i=1}^{m} y_{i}=0 ; \quad \frac{1}{m} \sum_{i=1}^{m} y_{i} y_{i}^{T}=I
$$
æ¥ç€å°†ç›®æ ‡å‡½æ•°çŸ©é˜µåŒ–ï¼š
$$
\begin{aligned}
J(Y) &=\sum_{i=1}^{m}\left\|y_{i}-\sum_{j=1}^{m} w_{i j} y_{j}\right\|_{2}^{2} \\
&=\sum_{i=1}^{m}\left\|Y I_{i}-Y W_{i}\right\|_{2}^{2} \\
&=\operatorname{tr}\left(Y(I-W)(I-W)^{T} Y^{T}\right)
\end{aligned}
$$
å…¶ä¸­ï¼ŒWä¸ºmxmçŸ©é˜µï¼Œå®ƒæ˜¯å¯¹å…¬å¼1ç»“æœçš„å±•å¼€ï¼Œå¯¹äºæ¯ [:,i] ç»´ï¼Œæœ‰kä¸ªè¿‘é‚»ä¸ºéé›¶å€¼ã€‚

æˆ‘ä»¬ä»¤ $M=(I-W)(I-W)^{T}$, åˆ™ä¼˜åŒ–å‡½æ•°è½¬å˜ä¸ºæœ€å°åŒ–ä¸‹å¼: $J(Y)=\operatorname{tr}\left(Y M Y^{T}\right)$ã€‚çº¦æŸå‡½æ•°çŸ©é˜µåŒ–ä¸º: $Y Y^{T}=m I$ã€‚å¼•å…¥lagrange multiplierï¼š
$$
L(Y)=\operatorname{tr}\left(Y M Y^{T}+\lambda\left(Y Y^{T}-m I\right)\right)\\
$$
å¯¹ $Y$ æ±‚å¯¼å¹¶ä»¤å…¶ä¸º 0 ï¼Œæˆ‘ä»¬å¾—åˆ° $2 M Y^{T}+2 \lambda Y^{T}=0$,å³ $M Y^{T}=\lambda^{\prime} Y^{T}$, è¿™æ ·æˆ‘ä»¬å°±å¾ˆæ¸…æ¥šäº†ï¼Œè¦å¾—åˆ°æœ€å°çš„ $\mathrm{d}$ ç»´æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºçŸ©é˜µ $\mathrm{M}$ æœ€å° çš„dä¸ªç‰¹å¾å€¼æ‰€å¯¹åº”çš„ $\mathrm{d}$ ä¸ªç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µ $Y=\left(y_{1}, y_{2}, \ldots y_{d}\right)^{T}$ å³å¯ã€‚ä½†æ˜¯ä¸LEç®—æ³•åŒæ ·çš„åŸå› ï¼Œæˆ‘ä»¬ä¸èƒ½å–Î»=0æ—¶å€™çš„ç‰¹å¾å‘é‡ã€‚æ‰€ä»¥å°±æ˜¯æœ€åå–çš„Yå°±æ˜¯ç¬¬2å°åˆ°ç¬¬d+1å°çš„ç‰¹å¾å‘é‡çš„æ‹¼æ¥ã€‚

##### æ•ˆæœ

ç®—æ³•æ­¥éª¤ï¼š

1. Compute the neighbors of each data point, $\vec{X}_{i}$.
2. Compute the weights $W_{i j}$ that best reconstruct each data point $\vec{X}_{i}$ from its neighbors, minimizing the cost in eq. (1) by constrained linear fits.
3. Compute the vectors $\vec{Y}_{i}$ best reconstructed by the weights $W_{i j}$, minimizing the quadratic form in eq. (2) by its bottom nonzero eigenvectors.

å¯¹æ¯”PCAæ•ˆæœè¿˜æ˜¯å¾ˆæ˜æ˜¾çš„ï¼š

![image-20211226232414624](media/DimensionReduction/image-20211226232414624.png)

å…·ä½“å¯ä»¥å»çœ‹æ–‡ä»¶å¤¹é‡Œçš„[è®ºæ–‡](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf)å’Œåšå®¢ã€‚

## é™„å½•

1. ä¸Šè¿°é™ç»´ç®—æ³•éƒ½æ˜¯æ— ç›‘ç£çš„æ–¹æ³•ï¼Œå½“æˆ‘ä»¬è¦å¯¹å¸¦æ ‡ç­¾çš„æ•°æ®è¿›è¡Œé™ç»´æ—¶ï¼Œåº”è¯¥é€‰æ‹©LDAï¼ˆLinear Discriminant Analysis  ï¼‰[çº¿æ€§åˆ¤åˆ«åˆ†æ](https://zhuanlan.zhihu.com/p/51769969)ç®—æ³•ï¼ˆæ–‡ä»¶å¤¹é‡Œæœ‰ç¬”è®°ï¼‰

2. å¯¹æ¯”å„ç§ç®—æ³•ï¼š

   https://towardsdatascience.com/reduce-dimensions-for-single-cell-4224778a2d67ï¼šIn this post, we have learnt that single cell genomics data have a **non-linear structure** which comes from the large proportion of **stochastic zeros** in the expression matrix due to the drop out effect. The **linear manifold learning** techniques preserve the **global structure** of the data and are not capable of fully resolving all cell populations present. In contrast, **preserving the** **local connectivity between the data points (LLE, tSNE, UMAP)** is the key factor for a successful dimensionality reduction of single cell genomics data.
   
   ![img](media/DimensionReduction/1U8RzOMplt76sbBJDCl8pBw.png)
   
   ![img](media/DimensionReduction/1Nf4tlA91a3TOcsvB3RAYTw.png)
   
   
   
   ![Overview and comparative study of dimensionality reduction techniques for  high dimensional data - ScienceDirect](media/DimensionReduction/1-s2.0-S156625351930377X-gr1.jpg)

3. éçº¿æ€§é™ç»´ï¼Œé€šå¸¸æœ€ä½³åº”ç”¨åœºæ™¯è¿˜æ˜¯åœ¨å¯è§†åŒ–ï¼Œå¯¹æ¯”éçº¿æ€§é™ç»´åå†æ¥çº¿æ€§åˆ†ç±»æˆ–å›å½’ä¸å¦‚ç›´æ¥æ¥ä¸€ä¸ªéçº¿æ€§çš„åˆ†ç±»/å›å½’å™¨ã€‚ç‰¹åˆ«æ˜¯t-SNEï¼Œç®—æ³•å¤æ‚åº¦æ¯”è¾ƒé«˜ï¼šhttps://www.zhihu.com/question/52022955/answer/387753267
